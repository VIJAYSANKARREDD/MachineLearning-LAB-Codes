{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ccc3e3-0054-4dca-8366-c9f79ab43b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running file: C:\\Users\\asus\\Downloads\\sleep-edf-database-expanded-1.0.0\\sleep-edf-database-expanded-1.0.0\\sleep-cassette\\SC4001E0-PSG.edf ===\n",
      "Loading PSG: C:\\Users\\asus\\Downloads\\sleep-edf-database-expanded-1.0.0\\sleep-edf-database-expanded-1.0.0\\sleep-cassette\\SC4001E0-PSG.edf\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Picked channel: EEG Fpz-Cz\n",
      "\n",
      "=== A1: Data Preparation ===\n",
      "Shape: (153, 7)\n",
      "Class distribution: Counter({3: 71, 2: 40, 1: 24, 0: 12, 4: 6})\n",
      "\n",
      "=== A2: Hyperparameter Tuning ===\n",
      "DecisionTree best params: {'max_depth': 3}\n",
      "RandomForest best params: {'n_estimators': 100, 'max_depth': 3}\n",
      "SVM best params: {'kernel': 'rbf', 'C': 10}\n",
      "LogReg best params: {'C': 0.1}\n",
      "\n",
      "=== A3: Comparative Evaluation ===\n",
      "\n",
      "DecisionTree:\n",
      "Train acc: 0.8333333333333334  Test acc: 0.5897435897435898\n",
      "Confusion matrix:\n",
      " [[ 0  1  0  2  0]\n",
      " [ 0  3  2  1  0]\n",
      " [ 1  0  7  2  0]\n",
      " [ 2  0  3 13  0]\n",
      " [ 0  1  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         3\n",
      "           1      0.600     0.500     0.545         6\n",
      "           2      0.538     0.700     0.609        10\n",
      "           3      0.722     0.722     0.722        18\n",
      "           4      0.000     0.000     0.000         2\n",
      "\n",
      "    accuracy                          0.590        39\n",
      "   macro avg      0.372     0.384     0.375        39\n",
      "weighted avg      0.564     0.590     0.573        39\n",
      "\n",
      "\n",
      "RandomForest:\n",
      "Train acc: 0.8245614035087719  Test acc: 0.717948717948718\n",
      "Confusion matrix:\n",
      " [[ 0  1  2  0  0]\n",
      " [ 0  3  1  2  0]\n",
      " [ 0  1  7  2  0]\n",
      " [ 0  0  0 18  0]\n",
      " [ 0  2  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         3\n",
      "           1      0.429     0.500     0.462         6\n",
      "           2      0.700     0.700     0.700        10\n",
      "           3      0.818     1.000     0.900        18\n",
      "           4      0.000     0.000     0.000         2\n",
      "\n",
      "    accuracy                          0.718        39\n",
      "   macro avg      0.389     0.440     0.412        39\n",
      "weighted avg      0.623     0.718     0.666        39\n",
      "\n",
      "\n",
      "SVM:\n",
      "Train acc: 0.7807017543859649  Test acc: 0.6410256410256411\n",
      "Confusion matrix:\n",
      " [[ 1  1  0  1  0]\n",
      " [ 0  3  2  1  0]\n",
      " [ 1  0  4  5  0]\n",
      " [ 0  0  1 17  0]\n",
      " [ 0  1  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     0.333     0.400         3\n",
      "           1      0.600     0.500     0.545         6\n",
      "           2      0.500     0.400     0.444        10\n",
      "           3      0.708     0.944     0.810        18\n",
      "           4      0.000     0.000     0.000         2\n",
      "\n",
      "    accuracy                          0.641        39\n",
      "   macro avg      0.462     0.436     0.440        39\n",
      "weighted avg      0.586     0.641     0.602        39\n",
      "\n",
      "\n",
      "LogReg:\n",
      "Train acc: 0.4649122807017544  Test acc: 0.46153846153846156\n",
      "Confusion matrix:\n",
      " [[ 0  0  0  3  0]\n",
      " [ 0  0  0  6  0]\n",
      " [ 0  0  0 10  0]\n",
      " [ 0  0  0 18  0]\n",
      " [ 0  0  0  2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         3\n",
      "           1      0.000     0.000     0.000         6\n",
      "           2      0.000     0.000     0.000        10\n",
      "           3      0.462     1.000     0.632        18\n",
      "           4      0.000     0.000     0.000         2\n",
      "\n",
      "    accuracy                          0.462        39\n",
      "   macro avg      0.092     0.200     0.126        39\n",
      "weighted avg      0.213     0.462     0.291        39\n",
      "\n",
      "\n",
      "=== A4: Regression Task Placeholder ===\n",
      "This section can include regression on continuous features (not implemented).\n",
      "\n",
      "=== A5: Clustering Task Placeholder ===\n",
      "This section can include clustering (KMeans, Hierarchical, etc.) (not implemented).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Lab07 A1–A5 — Classification on Sleep-EDF Dataset\n",
    "# Modularized code with fixes for consistent outputs\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ---------------------------\n",
    "# USER SETTINGS\n",
    "# ---------------------------\n",
    "DATA_PATH = r\"C:\\Users\\asus\\Downloads\\sleep-edf-database-expanded-1.0.0\"\n",
    "PREFERRED_CHANNELS = [\"EEG Fpz-Cz\", \"Fpz-Cz\", \"EEG Pz-Oz\", \"Pz-Oz\"]\n",
    "CROP_SECONDS = None             # None = full night, else crop in seconds\n",
    "MAX_EPOCHS_PER_FILE = 500       # limit epochs per subject\n",
    "EPOCH_SEC = 30                  # 30-second windows\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def load_raw(psg_file, hyp_file=None, verbose=True):\n",
    "    \"\"\"Load PSG EDF file + hypnogram annotations.\"\"\"\n",
    "    if verbose:\n",
    "        print(\"Loading PSG:\", psg_file)\n",
    "    raw = mne.io.read_raw_edf(psg_file, preload=True, stim_channel=None, verbose=\"ERROR\")\n",
    "    if CROP_SECONDS:\n",
    "        raw.crop(tmin=0, tmax=CROP_SECONDS)\n",
    "    if hyp_file and os.path.exists(hyp_file):\n",
    "        ann = mne.read_annotations(hyp_file)\n",
    "        raw.set_annotations(ann)\n",
    "    return raw\n",
    "\n",
    "def pick_best_channel(raw, preferred=PREFERRED_CHANNELS):\n",
    "    \"\"\"Pick EEG channel (prefer Fpz-Cz or Pz-Oz).\"\"\"\n",
    "    for p in preferred:\n",
    "        if p in raw.ch_names:\n",
    "            raw.pick_channels([p])\n",
    "            return p\n",
    "    # fallback\n",
    "    for ch in raw.ch_names:\n",
    "        if \"EEG\" in ch:\n",
    "            raw.pick_channels([ch])\n",
    "            return ch\n",
    "    raw.pick_channels([raw.ch_names[0]])\n",
    "    return raw.ch_names[0]\n",
    "\n",
    "def extract_epoch_features(raw, keep_stages=(2, 4), epoch_sec=EPOCH_SEC, max_epochs=MAX_EPOCHS_PER_FILE):\n",
    "    \"\"\"Extract epochs and compute statistical features.\"\"\"\n",
    "    mapping = {\n",
    "        \"Sleep stage W\": 0, \"W\": 0,\n",
    "        \"Sleep stage 1\": 1, \"1\": 1,\n",
    "        \"Sleep stage 2\": 2, \"2\": 2, \"N2\": 2,\n",
    "        \"Sleep stage 3\": 3, \"Sleep stage 4\": 3, \"3\": 3, \"4\": 3,\n",
    "        \"Sleep stage R\": 4, \"R\": 4, \"REM\": 4, \"REM sleep\": 4\n",
    "    }\n",
    "\n",
    "    sfreq = int(raw.info[\"sfreq\"])\n",
    "    sig = raw.get_data()[0]\n",
    "    features, labels = [], []\n",
    "\n",
    "    for i, ann in enumerate(raw.annotations):\n",
    "        if max_epochs and len(features) >= max_epochs:\n",
    "            break\n",
    "        desc = ann[\"description\"]\n",
    "        lab = mapping.get(desc, -1)\n",
    "        if lab == -1:  # ignore unknown\n",
    "            continue\n",
    "        start = int(ann[\"onset\"] * sfreq)\n",
    "        end = start + epoch_sec * sfreq\n",
    "        if end > len(sig):\n",
    "            continue\n",
    "        seg = sig[start:end]\n",
    "        feats = [\n",
    "            np.mean(seg), np.std(seg),\n",
    "            np.min(seg), np.max(seg),\n",
    "            np.percentile(seg, 25), np.percentile(seg, 75)\n",
    "        ]\n",
    "        features.append(feats)\n",
    "        labels.append(lab)\n",
    "\n",
    "    if not features:\n",
    "        return pd.DataFrame()  # empty\n",
    "\n",
    "    df = pd.DataFrame(features, columns=[\"mean\", \"std\", \"min\", \"max\", \"p25\", \"p75\"])\n",
    "    df[\"stage\"] = labels\n",
    "    return df\n",
    "\n",
    "# ---------------------------\n",
    "# A1: Data Preparation\n",
    "# ---------------------------\n",
    "def run_A1(df):\n",
    "    print(\"\\n=== A1: Data Preparation ===\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Class distribution:\", Counter(df[\"stage\"]))\n",
    "    return df.drop(columns=[\"stage\"]), df[\"stage\"]\n",
    "\n",
    "# ---------------------------\n",
    "# A2: Hyperparameter Tuning\n",
    "# ---------------------------\n",
    "def run_A2(X_train, y_train):\n",
    "    print(\"\\n=== A2: Hyperparameter Tuning ===\")\n",
    "    models = {\n",
    "        \"DecisionTree\": (DecisionTreeClassifier(), {\"max_depth\":[3,5,10,None]}),\n",
    "        \"RandomForest\": (RandomForestClassifier(), {\"n_estimators\":[50,100], \"max_depth\":[3,5,None]}),\n",
    "        \"SVM\": (SVC(), {\"C\":[0.1,1,10], \"kernel\":[\"linear\",\"rbf\"]}),\n",
    "        \"LogReg\": (LogisticRegression(max_iter=1000), {\"C\":[0.1,1,10]})\n",
    "    }\n",
    "    tuned_models = {}\n",
    "    for name, (model, param_grid) in models.items():\n",
    "        search = RandomizedSearchCV(model, param_grid, cv=3, n_iter=3, verbose=0)\n",
    "        search.fit(X_train, y_train)\n",
    "        tuned_models[name] = search.best_estimator_\n",
    "        print(f\"{name} best params:\", search.best_params_)\n",
    "    return tuned_models\n",
    "\n",
    "# ---------------------------\n",
    "# A3: Comparative Evaluation\n",
    "# ---------------------------\n",
    "def run_A3(models, X_train, y_train, X_test, y_test):\n",
    "    print(\"\\n=== A3: Comparative Evaluation ===\")\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        train_acc = model.score(X_train, y_train)\n",
    "        test_acc = model.score(X_test, y_test)\n",
    "        preds = model.predict(X_test)\n",
    "        report = classification_report(y_test, preds, digits=3, output_dict=True)\n",
    "        cm = confusion_matrix(y_test, preds)\n",
    "        results.append((name, train_acc, test_acc, report, cm))\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(\"Train acc:\", train_acc, \" Test acc:\", test_acc)\n",
    "        print(\"Confusion matrix:\\n\", cm)\n",
    "        print(classification_report(y_test, preds, digits=3))\n",
    "    return results\n",
    "\n",
    "# ---------------------------\n",
    "# A4: Regression Placeholder\n",
    "# ---------------------------\n",
    "def run_A4():\n",
    "    print(\"\\n=== A4: Regression Task Placeholder ===\")\n",
    "    print(\"This section can include regression on continuous features (not implemented).\")\n",
    "\n",
    "# ---------------------------\n",
    "# A5: Clustering Placeholder\n",
    "# ---------------------------\n",
    "def run_A5():\n",
    "    print(\"\\n=== A5: Clustering Task Placeholder ===\")\n",
    "    print(\"This section can include clustering (KMeans, Hierarchical, etc.) (not implemented).\")\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline\n",
    "# ---------------------------\n",
    "def main_run(psg_file, hyp_file):\n",
    "    raw = load_raw(psg_file, hyp_file)\n",
    "    ch = pick_best_channel(raw)\n",
    "    print(\"Picked channel:\", ch)\n",
    "    df = extract_epoch_features(raw, keep_stages=(2,4))\n",
    "    if df.empty:\n",
    "        print(\"No N2/REM found, falling back to ALL stages.\")\n",
    "        df = extract_epoch_features(raw, keep_stages=(0,1,2,3,4))\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"⚠️ Still no usable epochs, skipping file.\")\n",
    "        return\n",
    "\n",
    "    # A1\n",
    "    X, y = run_A1(df)\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "    # A2\n",
    "    tuned_models = run_A2(X_train, y_train)\n",
    "\n",
    "    # A3\n",
    "    results = run_A3(tuned_models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # A4\n",
    "    run_A4()\n",
    "\n",
    "    # A5\n",
    "    run_A5()\n",
    "\n",
    "def run_on_dataset(data_path, max_files=1):\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(data_path):\n",
    "        for f in files:\n",
    "            if f.endswith(\"-PSG.edf\"):\n",
    "                psg = os.path.join(root, f)\n",
    "                hyp = None\n",
    "                for g in files:\n",
    "                    if \"hyp\" in g.lower():\n",
    "                        hyp = os.path.join(root, g)\n",
    "                        break\n",
    "                print(\"\\n=== Running file:\", psg, \"===\")\n",
    "                main_run(psg, hyp)\n",
    "                count += 1\n",
    "                if max_files and count >= max_files:\n",
    "                    return\n",
    "    print(\"\\nLab07 run completed.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Run\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    run_on_dataset(DATA_PATH, max_files=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a94d79-e7d7-432c-bf61-b3b3dc1f2b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
